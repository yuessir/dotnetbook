## 流程规划

> 目前所有内容都将基于 Windows 操作系统进行讲解

为了理解如何按照不同线程的代码执行顺序，需要组织这些线程的执行计划。因为系统可能有一个或多个核心。如何模拟在一个核心上运行两个线程，或者不模拟这种情况。在每个核心上：无论是硬件核心还是模拟核心，都需要执行一个或多个线程。最终系统可能以虚拟化模式运行：在云端、虚拟机中、另一个操作系统的沙盒中。因此，我们必须强制性地考虑操作系统对线程的调度：.NET 平台根本不涉及这些问题，但在操作系统 .NET 中运行时，像任何其他应用程序一样，遵循该操作系统的规则。这些信息是多线程材料中非常重要的一部分，没有理解这些内容，多线程就无法在我们的头脑中正确地落实。

在 Windows 操作系统中，线程调度的组织是混合的。一方面，模拟了抢占式多任务条件，即操作系统自行决定何时以及根据何种条件抢占线程，以便给其他线程工作的机会（几乎所有应用程序的行为）。另一方面，是协作式多任务，即线程自行决定何时完成工作并切换到下一个（UMS 调度器，由用户代码手动创建。极少使用），或者使用可用的 `Thread.Yield()` 将剩余时间片让给其他线程。

多任务抢占模式基于优先级运行：调度程序根据它们的优先级决定*下一个*要执行的流。它是抢占式的，因为如果出现更高优先级的流，它会*立即*取代当前正在执行的流。然而，可以草率地得出结论，在许多情况下，这意味着由于它们的低优先级，一些流永远不会得到执行。然而，事实并非如此：操作系统有许多机制，允许低优先级的流获得执行时间。

### 优先级级别

Windows操作系统有32个执行流的优先级级别（0-31）：

- 1级别（00 - 00）-- Zero Page Thread;
- 15级别（01 - 15）-- 普通动态优先级;
- 16级别（16 - 31）-- 伪实时优先级。

![优先级级别](https://habrastorage.org/webt/my/cd/zj/mycdzj-0dxto8-mlmt8h_zb50mu.jpeg)

图1. 优先级级别 {.description}

优先级最低的是“Zero Page Thread”。这是操作系统的一个特殊线程，用于清零内存页面，从而清除那些不再需要的数据，因为页面已被释放（例如，通过GC释放了大量内存后，GC决定将一部分内存交还给操作系统）。这是有一个简单的原因的：当应用程序释放内存时（通常情况下我们不会清零类的字段），它可能会意外地将敏感数据交给他人。个人数据、密码等。因此，无论是操作系统还是编程语言的运行时环境（在我们这里是.NET CLR），都会清零分配给程序的内存区域。如果操作系统发现没有什么特别要做的：线程要么被阻塞等待某些事情，要么没有正在运行的线程，那么它会启动优先级最低的线程：内存清零线程。如果这个线程没有及时清零某些区域，也没关系：它们会按需清零：当这些区域被请求为可用时。但如果有时间，为什么不提前清零呢？

另外值得注意的是实时优先级，曾经很久以前被视为实时优先级，但很快失去了实时优先级的地位，只剩下了名义。换句话说，实时优先级实际上并不是如此，但仍然存在一些细微差别。根据操作系统的场景，可能会暂时增加线程的优先级（我们稍后会讨论这个问题）。然而，这种提升不能越过优先级15-16的边界：这是被禁止的。对我个人来说，很难想象出一种情况，需要这么高的优先级水平。特别是考虑到，对于系统来说，线程的概念是贯穿整个进程的。也就是说，提高系统中某个线程的优先级并不仅仅影响该进程，而是影响整个系统。总的来说，很难想象什么情况下确实需要相对于其他线程提高优先级。降低优先级很容易：某些后台服务中的进程完全可以按照这种情况运行，只有在有足够时间时才会执行。当负载没有完全占用核心时。

更糟糕的是，将某个线程的优先级提高相对于其他线程会增加其执行时间，是的。但也会减少其他线程的执行时间。这将导致.NET标准线程池指标下降，结果是（由于它不知道高优先级线程的存在）线程池开始增加线程以增加并行性水平。这反过来会导致线程池本身的增加“空闲时间”：它会越来越频繁地遇到自己的代码，试图从委托队列中获取下一个委托（队列是共享的，线程很多）。

### 从Windows API的角度看**进程**的优先级水平

优先级是一个*相对的*概念。这也是非常重要记住的。为了让我们更容易在优先级中导航，一些相对计算规则被引入：
- 线程的优先级在应用程序内部设置；
- 应用程序之间也有相对优先级；
- 但对于线程调度器而言，并非进程：它只处理线程的优先级；
- 这意味着由程序员设置的优先级并非最终的。最终的优先级是根据进程的优先级计算的。
- 因此，可能出现应用程序中具有低优先级的线程的提高优先级小于具有高优先级应用程序的降低优先级线程的情况（有时候油和醋...）。反之亦然。

但让我们按顺序来说。首先，对于调度器来说，所有应用程序的所有线程都是平等的。调度器看不到进程：它**只看到**线程。然后，当程序员编写自己的程序时，他明确或隐含地（使用默认值）为不同的线程设置优先级，从而在其应用程序内部创建多线程模型（在我们的.NET中，这是默认级别，几乎所有用户程序的线程都在此级别上运行）。他清楚地知道为什么在某个地方选择了降低优先级，在另一个地方选择了中等优先级，以及为什么有时会触发具有提高优先级的特殊线程。在应用程序内部，一切都被调整好了，这些值不会改变：即使进程的优先级发生变化。这被称为*相对优先级*。

然后，由于存在某个用户，他也可以为在操作系统上运行的*进程*设置优先级。例如，他可以为某个网络服务选择提高的优先级，从而为其分配最大资源（比如比防病毒软件更高）。因此，进程的优先级也可以被指定。这种优先级称为*优先级类*。

改变进程优先级不会改变应用程序内部线程的*相对优先级*：它们的最终值会发生偏移，但应用程序内部的优先级模型不会改变：仍然会有一个降低优先级的线程，一个普通优先级的线程和一个提高优先级的线程。这就是应用程序开发人员想要的。那么这是如何工作的呢？

存在6个*进程优先级类别*。进程优先级类别是创建线程优先级的相对参考。在更改某个进程的优先级时，可以在“任务管理器”中看到所有这些进程优先级类别。

|    名称         | 类别  | 基本优先级  |
|-----------------|--------|--------------------|
| 1. 实时         |   4    |    24              |
| 2. 高           |   3    |    13              |
| 3. 高于正常     |   6    |    10              |
| 4. 正常         |   2    |    8               |
| 5. 低于正常     |   5    |    6               |
| 6. 空闲         |   1    |    4               |

表1. 基本优先级级别 {.description}

换句话说，进程优先级类别是创建应用程序内部线程优先级的相对参考。为了设定基准，引入了*基本优先级*的概念。基本优先级是**值**，将成为具有**正常**优先级类型的线程的优先级：

- 如果进程以*正常类别*创建，并在该进程内创建具有*正常优先级*的线程，则其**实际正常优先级将为8**（表中的第4行）;
- 如果您创建一个进程，并且其*优先级类别为高于正常*，则*基本优先级*将为10。这意味着该进程内的线程将以更高的优先级创建：**正常不再是8，而是10**。

这是为什么呢？作为程序员，你们了解你们所使用的多线程模型。可能会有很多线程，你们决定其中一个线程应该是后台线程，因为它可能在计算某些指标或执行一些计算，而对于数据何时可用并不那么重要：重要的是这个线程*最终*完成计算（例如遍历和分析树的线程）。因此，你们将该线程的优先级设为较低。类似的情况可能发生在需要以较高优先级运行线程的情况下。

假设用户启动了你们的应用程序，并认为你们的应用程序消耗了太多处理器资源。用户认为你们的应用程序在系统中并不像其他某些应用程序那样重要，因此将你们的应用程序的优先级降低到Below Normal。这意味着用户设置了基本优先级为6，线程在你们的应用程序内部将根据这个基本优先级计算优先级。在应用程序中仍会有一个中等优先级的线程，一个降低优先级的线程和一个提高优先级的线程相对于这个中等优先级。但是在系统中，进程的优先级类别会下降，进程的线程优先级也会重新计算。它们会如何改变呢？

|                     | 优先级饱和级别  |
|---------------------|-------------------|
|   1. Time Critical  | (+15)             |
|   2. Highest        | (+2)              |
|   3. Above Normal   | (+1)              |
|   4. Normal         | (+0)              |
|   5. Below Normal   | (-1)              |
|   6. Lowest         | (-2)              |
|   7. Idle           | (-15)             |

表2. 优先级饱和级别

Normal 保持在相对于进程基本优先级的+0级别。Below Normal 是相对于基本级别的(-1)级别。也就是说，在我们将进程优先级降级到`Below Normal`类别的示例中，Below Normal 线程的优先级将重新计算为不是 `8 - 1 = 7`（在`Normal`类别时的优先级），而是 `6 - 1 = 5`。 Lowest (-2) 将变为 `4`。具有较高优先级 Highest 的线程将获得级别 `10 - 1 = 9`。

为什么“Normal”是`0`，相对于它只有两个步骤：-2、-1、+1和+2？这很容易与学习进行类比。我们去学校，得到我们知识的评分（5、4、3、2、1），我们明白这些评分代表什么：5是优秀，4是良好，3是没有努力，2是什么都没做，1是可以后来改成4。但如果我们引入了10分制评分系统（或者更糟糕的100分制），就会产生困惑：什么是9分或7分？如何知道给了你3还是4分？

同样的情况也适用于优先级。我们有“Normal”。然后，相对于“Normal”，我们有稍高于“Normal”（Normal above）和稍低于“Normal”（Normal below）。还有向上或向下两个步骤（Highest和Lowest）。相信我们，我们不需要更详细的分级。唯一很少会需要说的是：高于系统中的任何优先级。那么我们设定为“Time Critical”级别。或者相反：突然需要做某事，而整个系统都无事可做。那么我们设定为“Idle”级别。这些值被称为饱和级别。

### 优先级如何计算

![优先级级别](https://habrastorage.org/webt/rj/g2/qw/rjg2qw_-o8amgolaaxvh-sfzx-g.jpeg)

假设我们应用程序的进程优先级类别为Normal（表3），那么Normal线程的优先级为8。如果进程为Above Normal，则Normal线程的优先级为9。如果进程设置为Highest，则Normal线程的优先级为10。

由于对于Windows线程调度程序来说，所有进程的线程都是等价的，因此：

- 对于Normal类进程的Above-Normal线程；
- 对于Highest类进程的Normal线程。

最终优先级将是相同的，都是10。换句话说，系统将视它们的优先级为相同。

正如我们之前讨论过的那样，实时优先级组实际上并不是真正的实时，因为真正的实时是指消息在特定时间内被保证传递或者被处理。换句话说，如果在特定核心上有这样一个流，就不应该有其他流存在。然而，情况并非如此：系统可能会决定，低优先级流已经很久没有运行了，并给予它时间，关闭实时性。因此，更正确的说法是将其称为一个优先级类，该类在普通优先级之上运行，普通优先级无法进入该类，因为在Windows临时提高它们的优先级时会出现问题。

如果以图形方式表示，可以看到优先级类之间存在交叉。例如，Above-Normal、Normal和Below-Normal存在交叉（方框列）：

这意味着在这三个优先级类中，存在这样的线程优先级，使得它们的实际优先级是相同的。

### 线程状态

在规划术语中，流程可以处于多种状态之一，这些状态形成了一个有限状态机，流程在其中运行。当流程被创建时，它处于“初始化”状态。也就是说，它已被初始化并准备好工作。接着，当用户代码启动流程时，它被转换为“就绪”状态。换句话说，流程准备好执行。换句话说，流程准备好随时在任何处理器核心上开始执行。一段时间后，调度程序会唤醒以选择在当前处理器核心上执行的下一个流程（为每个核心创建一个调度程序）。为此，它遵循一系列规则：它会临时提高某些流程的优先级（稍后我们会详细讨论），有些流程处于阻塞状态，因此不参与调度（调度程序甚至不会考虑它们）。无论如何，一旦唤醒，它会从准备执行的流程列表中选择一个流程来切换到当前处理器核心。也就是说，那些在其状态机中处于“就绪”状态的流程。

当调度程序选择了一个流程后，它会将其从“就绪”状态的流程队列中移除，并将其转换为“待机”状态——一种中间状态。然后，调度程序必须在当前核心上替换活动流程。为此，它将处理器寄存器的状态设置为调度程序中断时的状态。这样一来，处理器就会回到调度程序中断时的计算状态。接着，它将流程状态更改为“运行”状态，并启动先前中断的代码。这样，当前核心上的活动流程就被替换为列表中的下一个流程。

每个线程都被分配一段时间，在这段时间内它可以执行，直到被中断以便让其他线程执行。这段时间被称为*时间片*。正是通过将时间划分为时间片，并且所有线程依次（逐渐地）执行，才产生了程序执行并行性的错觉。是的，在不同的核心上，代码是真正并行执行的。但在同一个核心内部，不是。正因为如此，如果处理器有多个核心，那么在它们上的代码可能会运行得更快。

当线程耗尽它的时间片时，调度器会将其转换为*就绪*或*等待*状态。线程进入*等待*状态的情况是，当它进入阻塞（核心级别的阻塞：互斥锁、信号量、与输入/输出磁盘系统的交互等）。当线程处于等待状态时，处理器根本不会花时间在该线程上，因为它从调度中消失了。当阻塞被解除（例如，`event.Set()`），线程被转换为就绪状态，然后在队列中排队后开始执行。还有一个额外的*转换*状态，但我们不会详细讨论。最后一个状态是*终止*—当线程的生命周期结束时。

操作系统为每个核心分配一个线程调度器（用于管理多个线程）。假设我们在**一个核心上**有50个*活跃*线程在执行。每个线程被分配的时间片因系统而异，例如在Windows操作系统的用户模式下可能为20毫秒。这意味着每个线程大约每1000毫秒（`50 * 20 = 1000 ms`）获得一次执行的机会。**也就是说，每秒一次，每次20毫秒**。因此，如果您创建大量进程，每个进程至少有一个活跃线程且没有被阻塞等待输入或输出，那么您不仅仅是在所有参与者之间分配总时间，还会推迟下一个线程被调度执行的时刻。

让我们总结一下。

线程的状态:

- 初始化。在创建线程时使用。
- 就绪。准备好。处于准备状态，等待执行。
  - 在寻找下一个线程时，调度程序只考虑这种状态。
- 延迟就绪。延迟执行的准备状态。
  - 线程已被选择在选定的处理器上执行，但尚未启动。
  - 为了最小化数据库调度的阻塞。
- 待机。处于增强就绪状态。
  - 已被选择在特定处理器上执行下一个。
  - 仍可能被更高优先级的线程取代。
- 运行。执行中。
  - 执行直到时间片结束。
- 等待。
  - 等待同步对象。
  - 操作系统等待线程的某些操作。
- 过渡。
  - 如果准备好执行，但核心堆栈已从内存中卸载。
- 终止。已结束。

图2：线程状态 {.description}

在图3中显示了线程和处理器之间的关系。可以看出，线程与处理器之间没有直接关系。对于线程调度器来说，它们都是等价的。

这一切是如何运作的呢？假设我们有三个进程1、2和4。有两组核心：每个核心都有自己的线程调度器。有一个优先级队列的线程。当线程被计划执行并进入就绪状态时，这意味着它将排入具有与线程本身优先级相匹配的线程队列。如果这是一个普通优先级的线程，在普通进程类应用程序中启动，那么它的优先级将为8。这意味着当它执行时，它将从运行状态转换为就绪状态，并排入队列8。如果还有其他线程也处于就绪状态，但具有更高的优先级，调度程序将首先执行它们，直到所有更高优先级的线程都被执行完毕（除非在某些情况下。例如，如果调度程序注意到某些线程长时间没有得到工作）。

图3：处理器和线程的交互 {.description}

### 时间片

量子是一个时间段，在这段时间内，从调度器启动或继续执行流的代码直到调度器决定切换到下一个流为止，流将持续执行。换句话说，量子是调度器分配给流用于连续执行的时间段。

第二个需要考虑的概念是系统定时器。系统定时器是一种硬件设备，其本质在于以非常准确且相同的范围向处理器的某个专门位置发送脉冲，从而引发*中断*。中断是通过在中断表中调用某个索引处的函数来调用操作系统代码。粗略地说，看起来大致如下：

```
Action<...>[] int_p = new Action<...>[256];

...

int_p[0x03]();            // 调用断点（IDE调试器函数）
int_p[0x08]();            // 系统定时器中断（来自IRQ0线）
int_p[0x10](arg1, arg2);  // 调用BIOS操作系统方法
int_p[0x21](arg1, arg2);  // 调用DOS操作系统方法
```

换句话说，当硬件定时器发出IRQ0信号时，它调用一个函数，其地址在程序中注册为int 0x08。这个函数本身是任何操作系统的线程调度器的调用函数：无论是Windows、Linux还是其他操作系统。在单处理器x86系统上，系统定时器的工作设置为每10毫秒触发一次，而在多处理器系统上则为每15毫秒一次。

图4：线程和处理器的时钟周期 {.description}

在计时器触发之间的范围中的一个第三部分被称为**量子单位**（见图4）。在一个核心上活动流的变化之间的量子单位数量被称为**量子重新启动值**（实际上在民间也称为量子）。这个值会根据操作系统及其版本的不同而有所不同：服务器版或用户版。在Windows客户端系统中，这个值为6。这意味着为了持续运行，分配给流的量子单位数量为6。因此，如果在Windows客户端版本上经过两个系统计时器节拍（在单处理器Windows上为10毫秒+10毫秒=20毫秒），考虑到每个节拍有3个量子单位，我们就得到了量子重新启动值=6。也就是说，在单处理器Windows客户端版本上，流每20毫秒变化一次，在多处理器Windows客户端版本上则为每30毫秒变化一次。

图5：量子单位 {.description}

- 量子目标 = \[实际量子单位\] \* \[每个量子时间的时钟脉冲\]

系统计时器的时间轴如图6所示。在Windows客户端系统中，量子重新启动值为6，而在Windows服务器系统中为36，也就是说长度是前者的6倍。这意味着为了持续运行，流将获得不是20毫秒，而是整整120毫秒。之后，它将被中断以执行同一核心上的另一个流。正因如此，在服务器系统中只需要保留那些必须在那里运行的应用程序，而不需要任何多余的内容：每个新的活动流都会延迟当前流的执行120毫秒。

图6：系统计时器时间轴 {.description}

  * 量子重新启动值 = \[实际量子单位\] \* \[每个量子时间的量子单位数量\]
    * = 6（2 \* 3）在Windows客户端系统中
    * = 36（12 \* 3）在Windows服务器系统中

假设在第二个量子单位级别上，一个线程被阻塞（见图6）。为了让处理器不空闲，这个线程失去了量子时间，被分配给另一个线程。另一个线程开始工作。但由于调度器只在系统定时器触发时运行，下一个线程将工作更长时间。接下来的线程将按照规定的时间工作。这意味着如果您在处理锁定，您的应用程序将只运行很少的时间。在线程运行时，可能会发生硬件中断（INT）。中断是将处理器从执行当前线程转移到执行更重要任务（如获取外部设备状态等）的操作。中断处理会占用线程的时间，但您不必担心，因为用于处理中断的时间不计入调度，因此您的线程将工作更长时间。然后，当从锁定中释放的线程唤醒时，它会完成其量子重新启动的价值。

让我们定义每个量子周期的量子周期数。根据以下步骤。找到量子周期的步骤：

- 我们来看一下处理器的时钟频率：
    - 执行<b>cpuid</b>命令，它将给出我们处理器的准确频率值（以GHz为单位）：GenuineIntel 2601 fffffff00000000 …
  - 将其转换为赫兹：
    - var freq = 2601 \* 1,000,000,000 = 2,601,000,000 次/秒
  - 使用Sysinternals的<b>clockres</b>工具获取系统中计时器的间隔值（计时器的触发频率）：
    - 15.625 毫秒 -- 这表示我们有一个多处理器系统（10 毫秒为单处理器系统）
  - 将其转换为秒：
    - 15.625 毫秒 / 1000 = 0.015625 秒
  - 将每秒的周期数乘以这个值，得到计时器间隔内的时钟数（从系统计时器的一个“滴答”到另一个“滴答”）：
    - 2,601,000,000 次/秒 \* 0.015625 秒 = 32,837,625 次
  - 得到一个量子单位的时钟数（量子单位是系统计时器的1/3）：
    - 32,837,625 / 3 = 10,945,875 次或 0x00A70553
  - 如果输出变量KiCyclesPerClockQuantum的转储，将得到类似的值：
    - lkd> dd nt!KiCyclesPerClockQuantum L1
    - 8149755c 00a70550

关于<b>量子单位</b>，目标是根据它来计算。这个变量存在于系统中，但只能在核心级别的调试器下获取。我们计算得到的值几乎与系统中的值相匹配（0x00A70550约等于0x00A70553）。对于我们的情况，最终数据如表5所示。它显示了在我们的进程中执行多少个时钟周期后会失去控制。

这很重要，有几个方面需要考虑：
  1. 如果有服务器应用程序并且有很多线程，而且它们组织得不太好，那么一些线程将因为其他线程而“受害”，如果它们在同一个核心上运行。
  2. 可以尝试将线程分配给某些核心，从而为它们提供更长时间的运行。
  3. 在锁定情况下将处理器时间传递给其他线程

| 系统    | 量子重启值 | 量子目标（时钟数） |
|------------|--------------------------|-------------------------|
| 客户端 |           6              |       65,675,250        |
| 服务器 |          36              |      394,051,500        |

表4. 重启值的时钟数 {.description}

#### 控制量子大小

打开计算机的属性。然后点击“更改设置”。

切换到性能 -\> 设置

切换到高级选项卡。程序--这是短量子（6）。为什么我们选择了6个量子作为桌面的设置呢？因为我们需要系统响应灵敏，因为有用户在使用计算机。假设在Chrome中打开了50个标签页。如果我们设置了一个等于36的量子（服务器），即使只是在屏幕上移动鼠标光标，GUI线程也会因为分配的时间太少而变得缓慢。

针对程序的优化
  * 使用短、可变的量子
  * 默认模式适用于客户端Windows，包括XBOX、Hololens
  * 如果服务器操作系统安装为桌面系统，可以选择此设置

针对后台服务的优化
  * 使用长、固定的量子
  * 默认模式适用于服务器Windows
  * 如果桌面操作系统安装为服务器系统，可以选择此设置
  * 由于效果立竿见影，可以在开始计算之前设置此选项，然后在计算结束后（例如早上）将设置还原

### 可变量子

时间量子不仅可以是静态的，还可以是可变的。静态量子是指无论条件如何，您都有6和36个量子。根据条件，可变量子可能会为您分配更多处理器时间。例如，当前位于前台的应用程序--它比其他所有应用程序运行时间更长，因此更快。在复制文件时，如果将鼠标光标移动到窗口上，复制速度会更快，因为这些线程会暂时提高优先级。

如果允许使用量子变量（客户端操作系统）：
* PspForegroundQuantum 被加载到量子变量表中，由 PspComputeQuantum 函数使用，PspVariableQuantums 表也被加载
* 算法根据进程是否在前台运行来选择适当的量子索引：
  * 是否有线程拥有前台窗口？
* 与线程调度器配对，这将提高该进程的优先级。

已启动两个计算器：右侧为活动（在前台），左侧为后台。有变量和固定量子组。表6显示了这两个计算器的索引。右侧将具有索引12或18，左侧为6。在用户 Windows 上，默认安装变量量子。在服务器上，默认情况下没有变量量子。

设置在注册表中更改。可以通过地址 `HKLM\SYSTEM\CurrentControlSet\PriorityControl` 更改量子分配系统。

参数按位分为三个区域。
短或长
* 0 或 3 - 客户端使用短，服务器使用长
* 1 - 长量子
* 2 - 短量子

变量或固定
* 0 或 3 - 客户端使用变量，服务器使用固定
* 1 - 变量量子

优先级分配
* 0 - 2 - 确定优先级分配

## 结论

1. 当一个线程进入阻塞状态时，它将不再参与调度。
2. 在单处理器系统中，调度器总是选择优先级最高的线程，因此它被称为首先具有优先级的，其次是抢占式的。如果出现一个优先级更高的线程，调度器将抢占当前执行的线程并切换到优先级更高的线程。
3. 重要的是要知道时间片。时间片是一个相对较长的时间过程。无论您的线程是否被中断，分配给线程的时间片都会被执行完毕。
4. 如果一个线程进入阻塞状态，一部分时间将被分配给另一个线程。