## 流程规划

> 目前所有内容都将基于 Windows 操作系统进行讲解

为了理解不同线程的代码应该以何种顺序执行，需要组织这些线程的执行计划。因为系统可能有一个或多个核心。无论是模拟两个核心在一个核心上，还是没有这种模拟。在每个核心上：无论是硬件核心还是模拟核心，都需要执行一个或多个线程。最终系统可能以虚拟化模式运行：在云端，在虚拟机中，在另一个操作系统的沙盒中。因此，我们必须强制性地考虑操作系统对线程的规划：.NET 平台根本不涉及这些问题，但在.NET操作系统中运行时，像任何其他应用程序一样，都遵循该操作系统的规则。这些信息是多线程材料中非常重要的一部分，没有理解这些内容，多线程绝对无法在我们的头脑中得到正确的位置。

在 Windows 操作系统中，线程规划的组织是混合的。一方面，模拟了抢占式多任务条件，当操作系统自行决定何时以及根据何种条件抢占线程，以便为其他线程提供工作时间（几乎所有应用程序的行为）。另一方面，是协作式多任务，当线程自行决定何时完成工作并切换到下一个（UMS调度程序，由用户代码手动创建。极少使用）或者我们可用的 `Thread.Yield()`，将剩余时间片让给其他线程。

多任务抢占模式基于优先级运行：调度程序根据它们的优先级决定*下一个*要执行的流。它是抢占式的，因为如果由于某种原因出现了更高优先级的流，它会*立即*取代当前正在执行的流。然而，可以匆忙地得出结论，在许多情况下，这意味着由于它们的低优先级，一些流永远不会被执行。但事实并非如此：操作系统中有许多机制，允许低优先级的流获得执行时间。

### 优先级级别

Windows操作系统有32个执行流的优先级级别（0-31）：

- 1级别（00 - 00）-- Zero Page Thread;
- 15级别（01 - 15）-- 普通动态优先级;
- 16级别（16 - 31）-- 伪实时优先级。

![优先级级别](https://habrastorage.org/webt/my/cd/zj/mycdzj-0dxto8-mlmt8h_zb50mu.jpeg)

图1. 优先级级别 {.description}

最低优先级（＃1）的是“Zero Page Thread”。这是操作系统的一个特殊线程，用于清零内存页面，从而清除那些不再需要的数据，因为页面已被释放（例如，通过GC释放了大量内存，然后决定将一部分交还给操作系统）。这是必要的一个简单原因：当应用程序释放内存时（通常不清零类字段），它可能无意中将敏感数据交给他人。个人数据，密码，等等。因此，无论是操作系统还是运行时编程语言（在我们这里是.NET CLR）都会清零分配给它们的内存区域。如果操作系统发现没有什么特别要做：线程要么处于等待状态，要么没有正在运行的线程，那么它会启动最低优先级的线程：内存清零线程。如果它无法及时清零某些区域，也没关系：它们会在需要时按需清零：当这些区域被请求为可用时。但如果有时间，为什么不提前清零呢？

另外值得注意的是实时优先级，曾经很久以前被视为实时优先级，但很快失去了实时优先级的地位，只剩下了名义。换句话说，实时优先级实际上并不是如此，但仍然存在一些细微差别。根据操作系统的场景，可以临时提高线程的优先级（我们稍后会讨论这个问题）。然而，这种提升不能越过15-16的优先级边界：这是被禁止的。对我个人来说，很难想象出一个情景，需要如此高的优先级水平。特别是考虑到，对于系统来说，线程的概念是贯穿整个进程的。也就是说，提高系统中某个线程的优先级并不只影响该进程，而是影响整个系统。总的来说，很难想象在什么情况下需要特别提高一个线程的优先级相对于其他线程。降低优先级很容易：某些后台服务中的进程可以按照这种情景运行，只有在有足够时间时才会执行。当负载没有完全占用核心时。

更糟糕的是，提高某个线程相对于其他线程的优先级会增加其执行时间，是的。但也会减少其他线程的执行时间。这将导致.NET标准线程池的指标下降，结果是（由于它不知道高优先级线程的存在）线程池开始增加线程以增加并行性水平。这反过来会导致线程池的“空闲时间”增加：它越来越频繁地遇到自己的代码，试图从共享队列中获取下一个委托（队列是共享的，线程很多）。

### 从Windows API的角度看**进程**的优先级水平

优先级是一个*相对的*概念。这也是非常重要记住的。为了让我们更容易在优先级中导航，一些相对计算规则被引入：
- 线程的优先级在应用程序内部设置；
- 应用程序之间也有相对优先级；
- 但对于线程调度器而言，它不涉及进程：它只处理线程的优先级；
- 这意味着由程序员设置的优先级并非最终的。最终的优先级是根据进程的优先级计算的。
- 因此，可能发生这样的情况，即具有低应用程序优先级的线程的提高优先级小于具有高应用程序优先级的线程的降低优先级（有点像油和醋混合…）。反之亦然。

但让我们按顺序来讨论。首先，对于调度器来说，来自所有应用程序的所有线程都是平等的。调度器看不到进程：它**只看到**线程。然后，当程序员编写他们的程序时，他们明确或隐含地（使用默认值）为不同的线程设置优先级，从而在其应用程序内部创建多线程模型（在我们的.NET中，这是默认级别，几乎所有用户程序的线程都在此级别上运行）。他们清楚地知道为什么在某个地方选择了降低优先级，在另一个地方选择了中等优先级，以及为什么有时会触发具有提高优先级的特殊线程。在应用程序内部，一切都被调整好了，这些值不会改变：即使进程的优先级发生变化。这被称为*相对优先级*。

此外，由于存在某些用户，他们也可以为在操作系统上运行的*进程*设置优先级。例如，他们可以为某个网络服务选择提高的优先级，从而为其分配最大资源（比如比防病毒软件更高）。因此，优先级水平也可以在进程级别上设置。这种优先级被称为*优先级类*。

改变进程优先级不会改变应用程序内部线程的*相对优先级*：它们的最终值会发生偏移，但应用程序内部的优先级模型不会改变：仍然会有一个降低优先级的线程，一个普通优先级的线程和一个提高优先级的线程。这正是应用程序开发人员想要的。那么这是如何工作的呢？

存在6个*进程优先级类*。进程优先级类是用于创建线程优先级的相对参考。在更改某个进程的优先级时，可以在“任务管理器”中看到所有这些进程优先级类。

|    名称         | 类别   | 基本优先级        |
|-----------------|--------|--------------------|
| 1. 实时         |   4    |    24              |
| 2. 高           |   3    |    13              |
| 3. 高于正常     |   6    |    10              |
| 4. 正常         |   2    |    8               |
| 5. 低于正常     |   5    |    6               |
| 6. 空闲         |   1    |    4               |

表1. 基本优先级级别 {.description}

换句话说，进程优先级类是用于确定应用程序内部线程优先级的相对参考。为了设定一个起点，引入了*基本优先级*的概念。基本优先级是指**普通**优先级类型的线程将具有的**值**：

- 如果进程以*正常类别*创建，并在该进程内创建一个*普通优先级*的线程，则其**实际普通优先级将为8**（表中的第4行）；
- 如果您创建一个进程并将其*优先级类别设为高于正常*，则*基本优先级*将为10。这意味着该进程内的线程将以更高的优先级创建：**普通将不再是8，而是10**。

这是为什么呢？作为程序员，你们了解你们所使用的多线程模型。可能会有很多线程，你们决定其中一个线程应该是后台线程，因为它可能在计算某些指标或执行某些计算，而对于数据何时可用并不那么重要：重要的是线程*最终*完成计算（例如遍历和分析树的线程）。因此，你们会将该线程的优先级降低。类似地，可能会出现需要以更高优先级运行线程的情况。

假设用户启动了您的应用程序，并认为您的应用程序消耗了太多处理器资源。用户认为您的应用程序在系统中并不像其他某些应用程序那样重要，因此将您的应用程序的优先级降低到Below Normal。这意味着用户设置了基本优先级为6，相对于这个基本优先级，您应用程序内部线程的优先级将被计算。在应用程序中，仍然会有一个中等优先级的线程，一个降低优先级的线程和一个提高优先级的线程相对于这个中等优先级。但是在系统中，进程的优先级类别会下降，进程的线程优先级也会重新计算。它们会如何改变呢？

|                     | 优先级饱和级别  |
|---------------------|-------------------|
|   1. Time Critical  | (+15)             |
|   2. Highest         | (+2)              |
|   3. Above normal   | (+1)              |
|   4. Normal         | (+0)              |
|   5. Below normal   | (-1)              |
|   6. Lowest         | (-2)              |
|   7. Idle           | (-15)             |

表2. 优先级饱和级别

Normal相对于进程的基本优先级保持在+0级别。Below normal是相对于基本级别的(-1)。也就是说，在我们的例子中，当将进程的优先级降低到`Below Normal`类别时，Below Normal线程的优先级将重新计算，不再是`8 - 1 = 7`（在`Normal`类别时的优先级），而是`6 - 1 = 5`。 Lowest (-2) 将变为`4`。具有较高优先级Highest的线程将获得级别`10 - 1 = 9`。

为什么“Normal”是`0`，相对于它只有两个步骤：-2、-1、+1和+2？这很容易与学习进行类比。我们去学校，得到我们知识的评分（5、4、3、2、1），我们明白这些评分代表什么：5--优秀，4--良好，3--基本没努力，2--什么都没做，1--可以之后改成4。但是如果我们引入了10分制（或者更糟糕的100分制），就会产生困惑：什么是9分或者7分？如何知道自己得到了3还是4分？

同样的道理也适用于优先级。我们有“Normal”。然后，相对于“Normal”，我们有稍高于“Normal”（Normal above）和稍低于“Normal”（Normal below）。还有向上或向下两个步骤（Highest和Lowest）。相信我们，我们并不需要更详细的分级。唯一很少情况下，可能一生中只有一次，我们需要说：比系统中的任何优先级都高。那么我们就设定为“Time Critical”级别。或者相反：突然间需要做某事，而整个系统都无事可做。那么我们就设定为“Idle”级别。这些值被称为饱和级别。

### 优先级如何计算

![优先级](https://habrastorage.org/webt/rj/g2/qw/rjg2qw_-o8amgolaaxvh-sfzx-g.jpeg)

假设我们应用程序的进程优先级类别为Normal（表3），那么Normal线程的优先级为8。如果进程为Above Normal，则Normal线程的优先级为9。如果进程为Higest，则Normal线程的优先级为10。

由于对于Windows线程调度程序来说，所有进程的线程都是等价的，因此：

- 对于Normal类别的进程和Above-Normal线程；
- 对于Higest类别的进程和Normal线程。

最终的优先级将是相同的，都是10。换句话说，系统将视它们的优先级为相同。

正如我们之前讨论过的那样，实时优先级组实际上并不是真正的实时，因为真正的实时是指消息在特定时间内被保证传递或接收到。换句话说，如果在特定核心上有这样一个流，那里就不应该有其他流。然而，事实并非如此：系统可能会决定，低优先级流已经很久没有运行了，并给予它时间，关闭实时功能。因此，更正确的说法是将其称为一个优先级类，该类在普通优先级之上运行，普通优先级无法进入该类，因为Windows会在临时情况下提高它们的优先级。

如果以图形方式表示，可以看到优先级类之间存在交叉。例如，存在 Above-Normal Normal Below-Normal 三个类的交叉（带有方块的列）。

这意味着对于这三个优先级类的进程，存在这样的线程优先级，使得它们的实际优先级是相同的。

### 线程状态

在规划术语中，流程可以处于多种状态之一，这些状态形成了一个有限状态机，流程在其中运行。当流程被创建时，它处于 *Initialized* 状态。也就是说，它已被初始化并准备好工作。接着，当用户代码启动流程时，它被转换为 *Ready* 状态。也就是说，准备好执行。换句话说，流程准备好随时在任何处理器核心上开始执行。一段时间后，调度程序会唤醒以选择在当前处理器核心上执行的下一个流程（为每个核心创建一个调度程序）。为此，它遵循一系列规则：它会临时提高某些流程的优先级（稍后我们会讨论这一点），有些流程被阻塞，因此不参与调度（调度程序甚至不会看它们）。无论如何，一旦唤醒，它会从准备执行的流程列表中选择一个流程，将当前处理器核心切换到该流程。也就是说，那些在其状态机中处于 *Ready* 状态的流程。

当调度程序选择了一个流程后，它会将其从 *Ready* 状态的流程队列中移除，并将其转换为 *Standby* 状态——一种中间状态。然后，调度程序必须在当前核心上替换活动流程。为此，它将处理器寄存器的状态设置为在调度程序中断时的状态。这样，处理器就会恢复到调度程序中断时的计算状态。接着，它将流程状态更改为 **Running** 状态，并启动先前中断的代码。这样，当前核心上的活动流程就被替换为列表中的下一个流程。

每个线程都被分配一段时间，在这段时间内它可以运行，直到被中断以便让其他线程运行。这段时间被称为*时间片*。正是因为将时间分割成时间片，并且所有线程依次（逐渐地）运行，才产生了程序执行并行性的错觉。是的，在不同的核心上，代码是真正并行执行的。但在同一个核心内部是不会的。正因为如此，如果处理器有多个核心，那么在这些核心上的代码可能会运行得更快。

当线程耗尽它的时间片时，调度器会将其转换为*就绪*或*等待*状态。线程进入*等待*状态的情况是，当它进入了一个阻塞状态（核心级别的阻塞：互斥锁、信号量、与输入/输出系统的交互等）。当线程处于等待状态时，处理器根本不会花时间在该线程上，因为它从调度中消失了。当阻塞被解除（例如，`event.Set()`），线程被转换为就绪状态，然后在队列中排队后开始执行。还有一个额外的*转换*状态，但我们不会详细讨论。最后一个状态是*终止*，表示线程的生命周期已结束。

操作系统为每个核心分配一个线程调度器（用于管理多个线程）。假设我们在**一个核心**上有50个*活跃*线程正在运行。一个线程被分配的时间片取决于系统，例如在Windows操作系统的用户模式下可能为20毫秒。这意味着每个线程大约每1000毫秒（`50 * 20 = 1000毫秒`）获得一次执行的机会。**也就是每秒一次，每次20毫秒**。因此，如果您创建大量进程，每个进程至少有一个活跃线程且没有被阻塞等待输入或输出，那么您不仅是在所有参与者之间分配总时间，还会推迟下一个线程开始工作的时刻。

让我们总结一下。

线程的状态:

- 初始化。在创建线程时使用。
- 就绪。准备好。准备状态，等待执行。
  - 在寻找下一个线程时，调度程序只考虑这种状态。
- 延迟就绪。准备延迟执行。
  - 线程已被选择在选定的处理器上执行，但尚未启动。
  - 为了最小化调度器对数据库的锁定。
- 待机。处于高度准备就绪状态。
  - 已被选定在特定处理器上执行下一个。
  - 仍可能被更高优先级的线程替换。
- 运行。执行中。
  - 执行直到时间片结束。
- 等待。
  - 等待同步对象。
  - 操作系统等待线程的某些操作。
- 过渡。
  - 如果准备就绪但核心堆栈已从内存中卸载。
- 终止。已结束。

第2张图片展示了线程的状态。

在第3张图片中展示了线程和处理器之间的关系。可以看出，线程与处理器之间没有直接关系。对于线程调度器来说，它们都是等价的。

这一切是如何运作的呢？假设我们有三个进程1、2和4。有两组核心：每个核心都有自己的线程调度器。有一个优先级队列的线程。当线程被计划执行并进入就绪状态时，这意味着它将排入具有与其自身优先级相匹配的线程队列。如果这是一个普通优先级的线程，在*进程类*为Normal的应用程序中启动，那么它的优先级将为8。这意味着当它执行完毕后，它将从运行状态转为就绪状态，并排入队列8。如果有其他线程也处于就绪状态，但具有更高的优先级，调度程序将首先执行它们，直到达到您的线程，除非所有具有更高优先级的线程都已执行（再次强调，除非某些情况下。例如，如果调度程序注意到某些线程太长时间没有得到工作）。

### 时间片段

量子是一个时间段，在这段时间内，从调度器启动或继续执行流的代码到调度器决定切换到下一个流的时刻，流将持续执行。换句话说，量子是调度器分配给流以连续执行的时间段。

第二个需要考虑的概念是系统定时器。系统定时器是一种硬件设备，其本质是以非常准确的相同范围向处理器的某个专门位置发送脉冲，从而引发*中断*。中断是通过在中断表中调用某个索引处的函数来调用操作系统代码。粗略地说，看起来大致如下：

```
Action<...>[] int_p = new Action<...>[256];

...

int_p[0x03]();            // 调用断点（IDE调试器函数）
int_p[0x08]();            // 系统定时器中断（来自IRQ0线）
int_p[0x10](arg1, arg2);  // 调用BIOS操作系统方法
int_p[0x21](arg1, arg2);  // 调用DOS操作系统方法
```

换句话说，当硬件定时器发送IRQ0信号时，它调用一个函数，其地址在程序中注册为int 0x08。这个函数本身是任何操作系统的线程调度器的调用函数：无论是Windows、Linux还是其他操作系统。在单处理器x86系统上，系统定时器设置为每10毫秒触发一次，而在多处理器系统上则为每15毫秒一次。

图4：系统定时器的时钟周期 {.description}

在定时器触发之间的范围的三分之一被称为**量子单位**（见图4）。在一个核心上活动流的变化之间的量子单位的数量被称为**量子重启值**（实际上在民间也称为量子）。这个值会根据操作系统及其版本的不同而有所不同：服务器版或用户版。在Windows客户端系统中，这个值为6。这意味着为了持续运行，分配给流的量子单位数量为6。因此，如果在Windows客户端版本上经过两个系统定时器节拍（在单处理器Windows上为10毫秒 + 10毫秒 = 20毫秒），考虑到每个节拍有3个量子单位，我们就得到了量子重启值=6。也就是说，在单处理器Windows客户端版本上，流每20毫秒变化一次，在多处理器Windows客户端版本上每30毫秒变化一次。

![量子单位](./img/5.png)

图5：量子单位 {.description}

- 量子目标 = \[实际量子单位\] \* \[每个量子时间的时钟脉冲\]

系统定时器的时间轴如图6所示。在Windows客户端系统中，量子重启值为6，而在Windows服务器系统中为36，也就是说是前者的6倍长。这意味着为了持续运行，流将不再是20毫秒，而是整整120毫秒。之后，它将被中断以执行同一核心上的另一个流。正因如此，在服务器系统中只需要保留那些必须在那里运行的应用程序，而不需要其他任何东西：每个新的活动流都会将当前的执行推迟120毫秒。

![系统定时器时间轴](./img/6.png)

图6：系统定时器时间轴 {.description}

  * 量子重启值 = \[实际量子单位\] \* \[每个量子时间的量子单位数量\]
    * = 6（2 \* 3）在Windows客户端系统中
    * = 36（12 \* 3）在Windows服务器系统中

假设在第二个量子单位级别上的一个线程被阻塞（见图6）。为了让处理器不空闲，该线程失去了量子时间，而这段时间被分配给另一个线程。另一个线程开始工作。但由于调度器只在系统定时器触发时才会启动，下一个线程将工作更长时间。接下来的线程将按照规定的时间工作。这意味着如果您使用阻塞，您的应用程序将只运行很少的时间。在线程运行时，可能会发生硬件中断（INT）。中断是将处理器从执行当前线程转移到执行更重要任务（如获取外部设备状态等）的过程。中断处理会占用线程的时间，但您不必担心，因为用于处理中断的时间不计入调度，因此您的线程将工作更长时间。接着，当解除阻塞的线程被唤醒时，它会继续执行其量子重新启动的值。

让我们定义每个量子周期的量子周期数。根据以下步骤。确定量子周期数的步骤：

- 首先我们来看处理器的时钟频率：
    - 执行 <b>cpuid</b> 命令，可以获取我们处理器的准确频率值（以GHz为单位）：GenuineIntel 2601 fffffff00000000 …
  - 将其转换为赫兹：
    - var freq = 2601 \* 1,000,000,000 = 2,601,000,000 次/秒
  - 使用 Sysinternals 的 <b>clockres</b> 工具获取系统中定时器的间隔值（定时器触发频率）：
    - 15.625 毫秒 -- 这表示我们有一个多处理器系统（10 毫秒为单处理器系统）
  - 将其转换为秒：
    - 15.625 毫秒 / 1000 = 0.015625 秒
  - 将每秒的循环次数乘以这个值，得到定时器间隔内的时钟数（系统定时器从一个“滴答”到另一个“滴答”所需的时钟数）：
    - 2,601,000,000 次/秒  \* 0.015625 秒 = 32,837,625 次
  - 得到每个量子单位的时钟数（量子单位是系统定时器的1/3）：
    - 32,837,625 / 3 = 10,945,875 次或 0x00A70553
  - 如果输出变量 KiCyclesPerClockQuantum 的转储值，会得到类似的数值：
    - lkd> dd nt!KiCyclesPerClockQuantum L1
    - 8149755c 00a70550

相对于 <b>量子单位</b> 来计算目标。这个变量存在于系统中，但只能在核心级别的调试器下获取。我们计算的值几乎与系统中的值相匹配（0x00A70550约等于0x00A70553）。对于我们的情况，最终数据在表5中呈现。它显示了在我们的进程中，在失去控制之前会执行多少个时钟周期。

这对于几个方面都很重要。
  1. 如果有服务器应用程序并且有大量线程，而且它们组织得不够好，那么一些线程会因为其他线程而“受苦”，如果它们在同一个核心上运行。
  2. 可以尝试将线程分配给某些核心，从而为它们提供更长时间的运行。
  3. 在锁定情况下将处理器时间传递给其他线程

| 系统    | 量子重启值 | 量子目标（时钟数） |
|------------|--------------------------|-------------------------|
| 客户端 |           6              |       65,675,250        |
| 服务器 |          36              |      394,051,500        |

表4. 重启值的时钟数 {.description}

#### 量子大小管理

打开计算机的属性。然后点击“更改设置”。

切换到性能 -\> 设置

切换到高级选项卡。程序 - 这是短量子（6）。为什么我们为桌面选择了6？因为我们需要系统响应速度，因为有用户在使用计算机。假设在Chrome中打开了50个标签页。如果我们设置量子为36（服务器），即使只是移动鼠标光标在屏幕上也会变得缓慢，因为GUI流程被分配的时间很少。

针对程序的优化
  * 使用短、可变量子
  * 适用于客户端Windows的默认模式，包括XBOX、Hololens
  * 如果服务器操作系统安装为桌面系统，可以选择设置

针对后台服务的优化
  * 使用长、固定量子
  * 适用于服务器Windows的默认模式
  * 如果桌面操作系统安装为服务器系统，可以选择设置
  * 由于效果立竿见影，可以在开始计算之前设置，然后在计算结束后（例如早上）将设置恢复原样

### 可变量子

时间量子不仅可以是静态的，还可以是可变的。静态量子是指在任何情况下都是6和36。根据情况，可变量子可以为您分配更多处理器时间。例如，前台应用程序会比其他应用程序运行更长时间，也更快。在复制文件时，如果将鼠标光标移动到窗口上，复制速度会更快，因为这些流程会暂时提高优先级。

如果允许使用量子变量（客户端操作系统）：
* PspForegroundQuantum 被加载到量子变量表中，由 PspComputeQuantum 函数使用，同时加载 PspVariableQuantums 表
* 算法根据进程是否在前台运行来选择适当的量子索引：
  * 进程中是否有拥有前台窗口的线程？
* 与线程调度器配合使用，这将提高该进程的优先级。

已启动两个计算器：右侧为活动计算器（在前台），左侧为后台计算器。存在量子组：变量和固定。表6显示了这些计算器的索引。右侧将具有索引12或18，左侧为6。在用户 Windows 上，默认情况下设置为变量量子值。在服务器上，默认情况下没有变量量子。

设置在注册表中更改。可以通过地址 `HKLM\SYSTEM\CurrentControlSet\PriorityControl` 更改量子分配系统。

参数按位分为三个区域。
短或长
* 0 或 3 - 客户端使用短，服务器使用长
* 1 - 长量子
* 2 - 短量子

变量或固定
* 0 或 3 - 客户端使用变量，服务器使用固定
* 1 - 变量量子

优先级分配
* 0 - 2 - 确定优先级分配

## 结论

1. 当一个线程进入阻塞状态时，它将不再参与调度。
2. 在单处理器系统中，调度器总是选择优先级最高的线程，因此它被称为首先具有优先级的，其次是抢占式的。如果出现一个优先级更高的线程，调度器将挤占当前执行的线程并切换到更高优先级的线程。
3. 重要的是要知道存在时间片。时间片是一个相对较长的时间段。无论您的线程是否被中断，分配给线程的时间片都会被使用完。
4. 如果一个线程进入阻塞状态，它的一部分时间将转移到另一个线程。