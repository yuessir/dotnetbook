It seems like you haven't provided any text to translate. Could you please provide the text you'd like translated into Chinese?

# 线程与线程池

> 🥤 *初级难度的材料*

本章涵盖了一些非常基础的问题的答案，同时立即深入到如何更好地工作的问题上：“应该使用单线程、多线程还是基于线程池的多线程？”初始时，这个问题的答案似乎非常简单和明了，然而实际情况完全不同：一切都极度依赖于具体情况：任务的类型、大小以及其他条件，这些并不是那么容易就能想到的。

因此，我们首先会讨论IO-/CPU-bound操作、创建线程的成本、线程池的基本工作原理（但仅限于基础），然后深入分析黑盒：线程池的性能依赖于什么？接受的工作量有多大才合适？

我们将以一些可能令人惊讶的结论结束本章，关于在线程池上保持应用程序高效运行的工作量。

## 在开始之前

关于多线程的章节应该从这样的思考开始：.NET中的所有线程和锁功能都是基于操作系统的功能。这意味着在.NET中没有自己的轻量级线程。但是，它提供了对操作系统线程和锁的封装，这些也深深根植于操作系统中。

因此，开始学习多线程之前，应该从操作系统级别开始，否则就没有关于线程工作算法的真实理解。对于一个普通的.NET开发者来说，线程更像是一个模糊的抽象，而不是一些可以理解的东西。因此，他们不理解什么是锁。什么是内核级同步和用户级同步，以及它们之间的区别。

所以，如果你是从这一章开始的，请回到前面的章节。之前的章节描述了非常重要的内容，这些内容将提升你的知识到一个新的水平（以及随之而来的工资水平）。

## Thread

什么是线程？让我们再次讨论。线程是处理器执行的一系列命令，它们以单个*线程*的形式并行或伪并行执行相对于其他*执行代码的线程*。并行，因为不同线程的代码可以在不同的物理核心上执行。伪并行，因为不同线程的代码可以在同一个物理核心上执行。因此，为了在用户眼中模拟并行，它们通过在执行时间上进行非常短的间隔切换，交替执行，从而创建了并行执行的错觉：这可以与彩色打印进行比较。如果你在放大镜下（或使用智能手机相机）观察全彩打印，你可以看到CMYK（青色、品红色、黄色、黑色）的微点。只有在放大下才能看到这些点，但从远处看，它们形成了最终颜色的单一斑点。

在.NET中，无法独占地占用处理器核心。操作系统也没有这样的功能。这意味着你的任何线程都可以在任何地方被中断：即使在`a = b;`操作的中间，当`b`已经被读取，但`a`还没有被写入，仅仅因为除了你之外还有其他人在同一个核心上工作。而且很有可能你会被中断更长的时间，比你被分配的工作时间还要长：在系统中有很多活动线程的情况下，除了你之外，在核心上还会有几个。这意味着你将会一点一点地执行，在某种队列中轮流。首先是你，然后是所有其他人，然后再次是你。

然而，创建线程是一个非常昂贵的操作。什么是“创建线程”？首先，这是对操作系统的调用。调用操作系统是跨越应用程序软件和操作系统层的障碍的操作。这些层由处理器提供，而障碍的两侧是*保护环*。应用程序软件拥有`Ring 3`保护环，而操作系统层占据`Ring 0`。从一个环到另一个环的方法调用是昂贵的操作，而且需要两次跨越：从`Ring 3`到`Ring 0`，然后返回。加上为线程创建堆栈：一个用于`Ring 3`，另一个用于`Ring 0`。加上创建.NET侧的额外数据结构。总之，为了并行执行一些操作*快速*，首先需要花费大量时间。

然而，人们注意到，很少有长时间连续执行而不等待硬件的操作。这更像是：
1. *等待客户端连接的网络请求*
2. 分析请求，形成数据库查询，发送
3. *等待数据库服务器的响应*
4. 收到回复，转换为服务响应
5. *发送响应*

而且，(2)和(4)的执行时间并不长。这更像是执行时间非常短的代码片段。因此，值得问一下：为什么要为它们创建单独的线程（这里指的是错误理解的*异步*和试图并行处理一切的尝试）？最终，(1)-(5)的链条完全是顺序的，这意味着在(1)、(3)和(5)点，执行线程处于等待硬件响应的阻塞状态，因为它在等待网络卡的响应。即，它不参与操作系统的调度，也不影响其性能。那么，web服务器是否需要为整个链条创建一个线程？如果服务器每秒处理1000个连接怎么办？我们知道，创建一个线程需要很长时间。这意味着它无法以这样的速度工作，如果它为每个请求创建一个线程。在已经存在的线程上工作？*租用线程？*

## ThreadPool

这就是为什么线程池ThreadPool的概念出现了，它解决了几个问题：
- 一方面，它抽象了线程的创建：我们不应该自己处理这个问题
- 创建一次线程后，它可以在其上执行完全不同的任务。你不在乎用哪个线程执行，只要有线程执行就行
    - 因此，我们不再浪费时间创建操作系统的线程：我们在已经创建的线程上工作
    - 因此，通过将我们的委托加载到ThreadPool中，我们可以均匀地将CPU核心的工作负载
- 或者限制吞吐量，或者相反：允许利用所有处理器核心的100%性能工作。

然而，我们稍后会发现，任何这样的抽象都有很多细节，在这些细节中，这种抽象工作得非常糟糕，可能成为严重问题的原因。线程池不能用于所有场景，在许多场景中，它会成为过程的“减速器”。然而，让我们从另一个角度来看这个问题。通常在解释某个主题时，作者会列出功能和可能性，但完全不解释“为什么”：好像一切都很明了。我们将尝试采取不同的方法。让我们试着理解ThreadPool。

如果从线程池是一个工具的角度来看，那么就会产生一个问题：这个工具应该解决什么问题？

### 从简化并行管理的角度来看

管理并行性并不简单。当然，这在很大程度上取决于你需要什么……但是当你“没有特别的需求”时，手动创建线程并通过它们推送任务流将变得过于复杂。更不用说这是一项相当常规的操作。因此，在类库中有标准功能看起来非常合理。

唯一的问题是，Microsoft团队错误地将ThreadPool做成了一个静态类，没有提供影响实现的可能性。这种限制造成了开发者对运行时开发者和最终开发者之间的一系列误解，因为后者没有感觉到可以开发自己的线程池，并在当前实现中使用它们。即，存在线程池的抽象，将它们交给外部算法使用，而不是使用标准的、通用的池。

但如果我们忽略这些缺陷（我们稍后会回到这个问题），那么从简化并行管理的角度来看，线程池当然是理想的解决方案。因为最终并行执行的代码只是一组需要并行执行的任务，它们并不关心在哪个线程上运行。它们需要并行执行。你甚至不太关心数量。你关心的是CPU被最优化地使用。

>{.big-quote} 当我们将ThreadPool视为*理想的*线程池时，它毫无问题地将任务从一个线程传递到另一个线程

### 从IO-/CPU-bound操作的角度来看

例如，可能会认为线程池是为了解决IO-/CPU-bound操作的分离而创建的。部分来说，这是正确的。我会说，线程池提供了分离IO-/CPU-bound操作的*第二层*机会。这个意思是，分离存在于没有它的更深层次上，即在操作系统级别上。

为了“找到”IO-/CPU-bound操作的第一层分离，我们需要回想一下线程是如何工作的。线程是操作系统对并行性的虚拟化层。这意味着处理器对线程一无所知，只有操作系统和在其上运行的所有东西才知道。此外，还存在锁，这些锁出于某种原因被所有人害怕（剧透：不必）。锁是等待操作系统信号的机制，比如某事发生了。例如，释放了信号量或释放了互斥锁。但是锁的列表是否仅限于同步原语？根据普通.NET开发者的知识，是的。但实际上并非如此。

让我们考虑导致阻塞状态的互斥方法调用。任何与操作系统的交互都会导致陷入更高的保护环（对于Windows是`Ring 0`，对于Linux是`Ring 2`），或者用应用程序软件的语言来说，进入内核空间。这意味着在陷入时会进行一系列操作，以隐藏操作系统数据免受应用程序软件层的影响。所有这些操作自然都是耗时的。从用户的角度来看，这个时间不是很长，因为用户思考得很慢。然而，从程序的角度来看，这个时间是相当明显的。接下来，如果谈到阻塞，就会触发一系列相当简单的机制，这些机制首先检查锁的状态：是否设置了锁，如果设置了，线程就从准备执行的队列转移到等待解锁的列表中。这意味着什么？由于操作系统在选择哪个线程在处理器上执行时遵循准备执行的线程列表，它不会注意到被锁定的线程，因此不会在它上面浪费资源。它根本不在计划中。因此，可以很容易地得出结论，当线程进入阻塞状态时，它就从执行计划中消失了，因此如果所有线程都处于阻塞状态，处理器的负载将为0%。

然而，是否只有同步原语具有这些属性？不：绝大多数IO-bound操作也具有相同的属性。是的，与硬件的交互可以在没有IO-bound操作的情况下进行。一个简单的例子可能是将某个设备的内存映射到操作内存的地址空间。例如，显卡：配置显卡后，通过调用BIOS函数将屏幕设置为所需的工作模式（文本、VESA图形等），除了将屏幕切换到所需模式外，BIOS还需要配置显卡与应用程序软件的交互。在文本模式下，BIOS会将显卡的地址空间映射到操作内存的0xB8000地址范围之后。通过写入这个地址的字符代码（当然，这适用于DOS或者如果你决定写你自己的操作系统，但不适用于Windows），你会立即在屏幕上看到它。这不会导致任何阻塞：写入将是瞬时的。

然而，在大多数设备（例如，网络卡、硬盘等）的情况下，你会有一个延迟来同步发送命令到设备（没有进入阻塞状态，但进入内核空间）和等待设备的响应。大多数这样的操作都伴随着将线程置于被阻塞状态。这是因为处理器和正在交互的设备之间的工作速度差异：当某个假设的网络卡正在发送数据包并进一步等待接收响应数据包时，在处理器核心上可以执行数百万、数十亿次操作。因此，当等待设备响应时，可以做很多其他事情，为了实现这一点，线程被置于阻塞状态并从计划中排除，从而为其他线程提供了使用处理器进行自己工作的机会。

从本质上讲，锁机制就是操作系统内置的IO-/CPU-bound操作分离机制。但它是否解决了所有分离任务？

每次进入阻塞状态都会降低IO-bound代码的并行级别。也就是说，当然，IO-bound和CPU-bound操作是并行的，但另一方面，当某个线程进入等待状态时，它就从计划中消失了，我们的应用程序就减少了对处理器的使用，因为从仅仅是CPU-bound操作的角度来看，我们*失去了*一个执行线程。而且已经从应用程序的角度，而不是操作系统的角度，可能会自然地希望用其他事情占用处理器。为了做到这一点，可以创建更多的线程并并行工作，但这将是一个相当繁重的任务，因此被发明了一个